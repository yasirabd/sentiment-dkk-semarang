{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Copy_of_Sentiment_Analysis_DKK_Semarang_(Word2Vec_Fasttext_&_Random_Forest).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apQ-1sWhsqH3",
        "outputId": "b2f517a6-34f3-4d64-f77f-c85519b39579"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install scikit-learn==1.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆ                              | 10 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–‰                            | 20 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 30 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=4cd2f490046729e52897e562b43d12b9b24126e251ff88f8d1e24b285bb1ae96\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n",
            "Requirement already satisfied: scikit-learn==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O73_3IGSiStg"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPvndx4f658F",
        "outputId": "a0439e03-8c21-46a9-835c-6a0dad180b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM2i-RdoiTy7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string, re, requests, csv\n",
        "from google.colab import drive\n",
        "from wordcloud import WordCloud\n",
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TUbZKrJiVOU",
        "outputId": "f2c1d13a-5429-466d-a649-2c1231c7aab3"
      },
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_ovLo6EiX2_"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8qMho9XsiWdX",
        "outputId": "9eb6f5a8-ff4f-49d9-f7d2-1306d3814b7d"
      },
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>username</th>\n",
              "      <th>likes</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ðŸ”¥</td>\n",
              "      <td>negative</td>\n",
              "      <td>lanyardsemarang</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-11-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Naik Turun maju can5ik ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
              "      <td>neutral</td>\n",
              "      <td>hadi_soeparno</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-03-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Prosentase tingkat kematian karena covid 19 ut...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>heryadisaputro</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-09-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Rapid test/ test swab PCR Di DKK/dinkes Semara...</td>\n",
              "      <td>negative</td>\n",
              "      <td>airlangga15</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-05-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Min, area pedurungan bs dilakukan dmn,</td>\n",
              "      <td>neutral</td>\n",
              "      <td>fauzanabell</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-06-22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...    datetime\n",
              "0                                                  ðŸ”¥  ...  2020-11-02\n",
              "1                        Naik Turun maju can5ik ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚  ...  2021-03-29\n",
              "2  Prosentase tingkat kematian karena covid 19 ut...  ...  2020-09-22\n",
              "3  Rapid test/ test swab PCR Di DKK/dinkes Semara...  ...  2020-05-29\n",
              "4             Min, area pedurungan bs dilakukan dmn,  ...  2021-06-22\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSpz3SrTigew"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrLHGvjxjNP_"
      },
      "source": [
        "## Wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbVXS7syidAd"
      },
      "source": [
        "# positive comments before preprocessing\n",
        "# data_pos = train[train['label'] == 'positive']\n",
        "\n",
        "# all_text = ' '.join(word for word in data_pos['text'])\n",
        "# wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text)\n",
        "# plt.figure(figsize=(20,10))\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.axis(\"off\")\n",
        "# plt.margins(x=0, y=0)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9yRzPoYimIE"
      },
      "source": [
        "# negative comments before preprocessing\n",
        "# data_neg = train[train['label'] == 'negative']\n",
        "\n",
        "# all_text = ' '.join(word for word in data_neg['text'])\n",
        "# wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text)\n",
        "# plt.figure(figsize=(20,10))\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.axis(\"off\")\n",
        "# plt.margins(x=0, y=0)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olxg2IjAjByz"
      },
      "source": [
        "# neutral comments before preprocessing\n",
        "# data_neut = train[train['label'] == 'neutral']\n",
        "\n",
        "# all_text = ' '.join(word for word in data_neut['text'])\n",
        "# wordcloud = WordCloud(colormap='Blues', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text)\n",
        "# plt.figure(figsize=(20,10))\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.axis(\"off\")\n",
        "# plt.margins(x=0, y=0)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBmjPgdt_Obw",
        "outputId": "3ed96531-7f0c-4662-8281-1dbfe32bba02"
      },
      "source": [
        "# value counts\n",
        "train['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral     2926\n",
              "negative    2775\n",
              "positive    2266\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vygorxeojRKo"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-RvDQEljeEV"
      },
      "source": [
        "# train_text = train['text']\n",
        "# test_text = test['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pra6PprkPoWV"
      },
      "source": [
        "# # CLEANSING\n",
        "# def cleansing(data):\n",
        "\n",
        "#     # lowercasing\n",
        "#     data = data.lower()\n",
        "\n",
        "#     # remove punctuation\n",
        "#     punct = string.punctuation\n",
        "#     translator = str.maketrans(punct, ' '*len(punct))\n",
        "#     data = data.translate(translator)\n",
        "\n",
        "#     # remove ASCII dan unicode\n",
        "#     # data = data.encode('ascii', 'ignore').decode('utf-8')\n",
        "#     # data = re.sub(r'[^\\x00-\\x7f]',r'', data)\n",
        "    \n",
        "#     # remove newline\n",
        "#     data = data.replace('\\n', ' ')\n",
        "\n",
        "#     # remove digit\n",
        "#     pattern = r'[0-9]'\n",
        "#     data = re.sub(pattern, '', data)\n",
        "\n",
        "#     # remove extra space\n",
        "#     data = ' '.join(data.split())\n",
        "    \n",
        "#     return data\n",
        "\n",
        "# import sys\n",
        "# # REMOVE EMOJI\n",
        "# # def remove_emoji(data):\n",
        "# #     emoji_pattern = re.compile(\"[\"\n",
        "# #                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "# #                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "# #                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "# #                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "# #                            u\"\\U00002702-\\U000027B0\"\n",
        "# #                            u\"\\U000024C2-\\U0001F251\"\n",
        "# #                            \"]+\", flags=re.UNICODE)\n",
        "# #     return emoji_pattern.sub(r' ', data)\n",
        "\n",
        "# # CONVERT EMOJIS\n",
        "# import emoji\n",
        "# import functools\n",
        "# import operator\n",
        "# import re\n",
        "\n",
        "# df_emoji = pd.read_csv('/content/emoji_to_text.csv')\n",
        "# UNICODE_EMO = {row['emoji']:row['makna'] for idx,row in df_emoji.iterrows()}\n",
        "# def convert_emojis(text):\n",
        "#     # split emojis\n",
        "#     em_split_emoji = emoji.get_emoji_regexp().split(text)\n",
        "#     em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
        "#     em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
        "#     text = ' '.join(em_split)\n",
        "\n",
        "#     # convert emojis\n",
        "#     for emot in UNICODE_EMO:\n",
        "#         text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
        "#     return text.lower()\n",
        "\n",
        "# # CONSTRUCT KAMUS ALAY\n",
        "# text_path1 = 'https://raw.githubusercontent.com/ramaprakoso/analisis-sentimen/master/kamus/kbba.txt'\n",
        "# text_path2 = 'https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv'\n",
        "# kamus_alay1 = pd.read_csv(text_path1, delimiter=\"\\t\", header=None, names=['slang', 'formal'])\n",
        "# kamus_alay2 = pd.read_csv(text_path2)\n",
        "# kamus_alay = pd.concat([kamus_alay1, kamus_alay2[['slang', 'formal']]]).reset_index(drop=True)\n",
        "\n",
        "# dict_alay = dict()\n",
        "# for index, row in kamus_alay.iterrows():\n",
        "#     dict_alay[row['slang']] = row['formal']\n",
        "\n",
        "# # NORMALIZE COLLOQUIAL/ALAY\n",
        "# def normalize_text(data):\n",
        "#   word_tokens = word_tokenize(data)\n",
        "#   result = [dict_alay.get(w,w) for w in word_tokens]\n",
        "#   return ' '.join(result)\n",
        "\n",
        "\n",
        "# # CONSTRUCT STOPWORDS\n",
        "# rama_stopword = \"https://raw.githubusercontent.com/ramaprakoso/analisis-sentimen/master/kamus/stopword.txt\"\n",
        "# yutomo_stopword = \"https://raw.githubusercontent.com/yasirutomo/python-sentianalysis-id/master/data/feature_list/stopwordsID.txt\"\n",
        "# fpmipa_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/fpmipa-stopwords.txt\"\n",
        "# sastrawi_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/sastrawi-stopwords.txt\"\n",
        "# aliakbar_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/aliakbars-bilp.txt\"\n",
        "# pebahasa_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-list/pebbie-pebahasa.txt\"\n",
        "# elang_stopword = \"https://raw.githubusercontent.com/onlyphantom/elangdev/master/elang/word2vec/utils/stopwords-id.txt\"\n",
        "# nltk_stopword = stopwords.words('indonesian')\n",
        "\n",
        "# path_stopwords = [rama_stopword, yutomo_stopword, fpmipa_stopword, sastrawi_stopword, \n",
        "#                   aliakbar_stopword, pebahasa_stopword, elang_stopword]\n",
        "\n",
        "# # CUSTOM STOPWORDS\n",
        "# other = '''\n",
        "# admin mimin min minkes kalo nya username\n",
        "# '''\n",
        "\n",
        "# # gabungkan stopwords\n",
        "# stopwords_l = nltk_stopword\n",
        "# for path in path_stopwords:\n",
        "#     response = requests.get(path)\n",
        "#     stopwords_l += response.text.split('\\n')\n",
        "\n",
        "# st_words = set(stopwords_l)\n",
        "# other_stopword = set(other.split())\n",
        "\n",
        "# stop_words = st_words | other_stopword\n",
        "\n",
        "# # REMOVE STOPWORDS\n",
        "# def remove_stopword(text, stop_words=stop_words):\n",
        "#     word_tokens = word_tokenize(text)\n",
        "#     filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "#     return ' '.join(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE26lhvxQT8-"
      },
      "source": [
        "# # full pipeline preprocess\n",
        "# def preprocess(data):\n",
        "#     data = cleansing(data)\n",
        "#     # data = remove_emoji(data)\n",
        "#     data = convert_emojis(data)\n",
        "#     data = normalize_text(data)\n",
        "#     data = remove_stopword(data)\n",
        "#     return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9iH2W-dQfVy"
      },
      "source": [
        "# # rename username to @username\n",
        "# pattern = \"(?:@)([A-Za-z0-9_](?:(?:[A-Za-z0-9_]|(?:\\.(?!\\.))){0,28}(?:[A-Za-z0-9_]))?)\"\n",
        "# train_text = train_text.apply(lambda x: re.sub(pattern, \"@username\", x))\n",
        "# test_text = test_text.apply(lambda x: re.sub(pattern, \"@username\", x))\n",
        "\n",
        "# # preprocess\n",
        "# train_text = train_text.apply(lambda x: preprocess(x))\n",
        "# test_text = test_text.apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQCppfUgRDpD"
      },
      "source": [
        "# train_text.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp0SpxL2V9fo"
      },
      "source": [
        "# import random\n",
        "# id = random.randint(0,len(train_text))\n",
        "\n",
        "# train['text'][id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xbGp4h_TN9y"
      },
      "source": [
        "# text = '''Makasih infonya.....ayo jgn lengah......tetap smangat... pandemi belom usai...\n",
        "\n",
        "# ...tetap ikuti protokol kesehatan dengan baik....ðŸ™ðŸ™ðŸ™'''\n",
        "# text = cleansing(text)\n",
        "# text = convert_emojis(text)\n",
        "# text = normalize_text(text)\n",
        "# text = remove_stopword(text)\n",
        "# text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnm8Fp0RWzoz"
      },
      "source": [
        "# dict_alay.get('jgn','a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxpRrMG7lBWN"
      },
      "source": [
        "# Feature extraction (Word2Vec Fasttext)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilEt5X-2TFdd"
      },
      "source": [
        "# # dowload pre-trained word2vec fasttext indonesia\n",
        "# ! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz\n",
        "# # unzip\n",
        "# ! gunzip cc.id.300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgo_mSNlTFmf"
      },
      "source": [
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.models.wrappers import FastText\n",
        "\n",
        "# # load pre-trained word2vec fasttext\n",
        "# word2vec = FastText.load_fasttext_format('cc.id.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjFqBV8STFrv"
      },
      "source": [
        "# # check vocab pada pre-trained model\n",
        "# vocab = word2vec.wv.vocab\n",
        "# words = word2vec.wv.index2word\n",
        "\n",
        "# print(f'vocabulary length: {len(vocab)}')\n",
        "# print(f'top 20 words in word2vec:\\n{words[:20]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU62kF7DgFtB"
      },
      "source": [
        "# get word vector 'makan'\n",
        "# word2vec['makan']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIbzJt4f3aTo"
      },
      "source": [
        "# # tokenize text\n",
        "# def tokenize(sentence):\n",
        "#     return word_tokenize(sentence)\n",
        "\n",
        "# train_text = train_text.apply(lambda x: tokenize(x))\n",
        "# test_text = test_text.apply(lambda x: tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwL-iIH1wUDS"
      },
      "source": [
        "# train_text[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q39RgdCVmHpS"
      },
      "source": [
        "# # vectorize\n",
        "# VOCABULARY = word2vec.wv.vocab\n",
        "# def vectorize(tokens):\n",
        "#     word_vec = []\n",
        "#     for w in tokens:\n",
        "#         if w in VOCABULARY:\n",
        "#             # get word vector from pre-trained word2vec fasttext\n",
        "#             word_vec.append(word2vec[w])\n",
        "#     return word_vec\n",
        "\n",
        "# def avg_vectorize(tokens):\n",
        "#     sum_vec = np.zeros(300)\n",
        "#     word_count = 0\n",
        "\n",
        "#     for w in tokens:\n",
        "#         if w in VOCABULARY:\n",
        "#             # word vector from pre-trained word2vec fasttext and add vector\n",
        "#             sum_vec += word2vec[w]\n",
        "#             word_count += 1\n",
        "#     return sum_vec if word_count==0 else sum_vec/word_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDW2hKWkMzeR"
      },
      "source": [
        "# # check maximum token and count\n",
        "# max_size_token = 0\n",
        "# count = 0\n",
        "# max_len_choosen = 50\n",
        "\n",
        "# for tok in train_text:\n",
        "#     # find max token\n",
        "#     if len(tok) > max_size_token: max_size_token = len(tok)\n",
        "#     # count\n",
        "#     if len(tok) > max_len_choosen: count += 1\n",
        "\n",
        "# print(f'Maximum length token: {max_size_token}')\n",
        "# print(f'With MAX_LEN {max_len_choosen}, there are/is {count} token/s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SokY7xpVJRFt"
      },
      "source": [
        "# # padding\n",
        "# MAX_LEN = 50\n",
        "\n",
        "# def add_padding(word_vec):\n",
        "#     if len(word_vec) < MAX_LEN:\n",
        "#         pad_count = MAX_LEN - len(word_vec)\n",
        "#         return word_vec + [np.array([0]*300)]*pad_count\n",
        "#     else:\n",
        "#         return word_vec[:MAX_LEN]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6N3CmSgUJxN"
      },
      "source": [
        "# # feature extraction\n",
        "# def extract_feature(data, ndim=3):\n",
        "#     if ndim == 3:\n",
        "#         features = vectorize(data)\n",
        "#         features = add_padding(features)\n",
        "#     elif ndim == 2:\n",
        "#         features = avg_vectorize(data)\n",
        "#     return np.array(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtp6l2jgZadv"
      },
      "source": [
        "# extract feature\n",
        "# train_features = np.array([extract_feature(text, ndim=2) for text in train_text])\n",
        "# test_features = np.array([extract_feature(text, ndim=2) for text in test_text])\n",
        "\n",
        "# train_features.shape, test_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export\n",
        "# np.save(\"train_rf_wordvec.npy\", train_features)\n",
        "# np.save(\"test_rf_wordvec.npy\", test_features)"
      ],
      "metadata": {
        "id": "eZg8GcC4zTc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_train = \"/content/drive/MyDrive/5. Research & Development/Kepengurusan 2021-2022/1 - Research Instagram DKK Semarang/Code and Model/train_rf_wordvec.npy\"\n",
        "path_test = \"/content/drive/MyDrive/5. Research & Development/Kepengurusan 2021-2022/1 - Research Instagram DKK Semarang/Code and Model/test_rf_wordvec.npy\"\n",
        "train_features = np.load(path_train)\n",
        "test_features = np.load(path_test)\n",
        "\n",
        "train_features.shape, test_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRk2q7xBzjeO",
        "outputId": "e96df6a0-c74f-4792-ab4c-bfc6a59f6bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7967, 300), (1992, 300))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC0YRSYicJfW"
      },
      "source": [
        "NOTE:\n",
        "- Metode neural network/deep learning seperti LSTM, CNN bisa menerima feature input 3 dimensi.\n",
        "- Metode SVM tidak bisa menerima input 3 dimensi, sehingga fitur harus dirubah menjadi 2 dimensi. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B008EKh1lmpc"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N73a0FT2mPJ7"
      },
      "source": [
        "# mapping label\n",
        "mapper = {'neutral':0, 'positive':1, 'negative':2}\n",
        "train_y = train['label'].map(mapper)\n",
        "test_y = test['label'].map(mapper)\n",
        "\n",
        "# train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF5fY4xylkNK",
        "outputId": "8726585c-9de8-4d37-b400-a767376d45b1"
      },
      "source": [
        "# from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(train_features, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjfTei6gnLhr",
        "outputId": "8fee0635-9b0f-401b-8568-b200b23112c8"
      },
      "source": [
        "%%time\n",
        "# cross-val score\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_y, cv=5)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.66562108 0.67001255 0.68298807 0.69052103 0.66729441]\n",
            "CPU times: user 45.1 s, sys: 90 ms, total: 45.2 s\n",
            "Wall time: 47.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.get_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SpdCHnXV9bG",
        "outputId": "ce605af0-feeb-4a99-e662-d706962b4d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'class_weight': None,\n",
              " 'criterion': 'gini',\n",
              " 'max_depth': None,\n",
              " 'max_features': 'auto',\n",
              " 'max_leaf_nodes': None,\n",
              " 'max_samples': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'oob_score': False,\n",
              " 'random_state': None,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HBxvT4ql8P_"
      },
      "source": [
        "# predict\n",
        "y_pred = clf.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymq98Z6TmAGF",
        "outputId": "f757b877-5ee2-411d-d17c-ca513c831a46"
      },
      "source": [
        "# accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(test_y, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6822289156626506"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPA1vXVf-TYx"
      },
      "source": [
        "# from joblib import dump, load\n",
        "\n",
        "# dump(clf, 'word2vec_svc.joblib') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Randomized Search\n",
        "# references: \n",
        "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "            #    'max_features': max_features,\n",
        "            #    'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf_random = RandomizedSearchCV(estimator=clf, \n",
        "                                param_distributions=random_grid, \n",
        "                                n_iter=10, \n",
        "                                cv=5, \n",
        "                                random_state=2021)\n",
        "# Fit the random search model\n",
        "clf_random.fit(train_features, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q0_vDtR7w0i",
        "outputId": "35b8f502-c8a7-41be-ee9f-fb3b30309f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1h 57min 29s, sys: 9.39 s, total: 1h 57min 38s\n",
            "Wall time: 1h 57min 5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_random.best_params_"
      ],
      "metadata": {
        "id": "Lx5W2C3H7w2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989986a8-7094-4b16-89fc-321a965f9222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': False,\n",
              " 'min_samples_leaf': 4,\n",
              " 'min_samples_split': 10,\n",
              " 'n_estimators': 2000}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf_random.predict(test_features)\n",
        "\n",
        "accuracy = accuracy_score(test_y, y_pred)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "Qcyixbq97w3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bf6b71-a819-422b-a4ef-765bd3123851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6852409638554217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# cross-val score\n",
        "scores = cross_val_score(clf_random, train_features, train_y, cv=5)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "AYHsn96UDQTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_random.get_params()"
      ],
      "metadata": {
        "id": "a_HhOX367w58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362830ea-f0c7-4d81-9131-5634b7aa0e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cv': 5,\n",
              " 'error_score': nan,\n",
              " 'estimator': RandomForestClassifier(),\n",
              " 'estimator__bootstrap': True,\n",
              " 'estimator__ccp_alpha': 0.0,\n",
              " 'estimator__class_weight': None,\n",
              " 'estimator__criterion': 'gini',\n",
              " 'estimator__max_depth': None,\n",
              " 'estimator__max_features': 'auto',\n",
              " 'estimator__max_leaf_nodes': None,\n",
              " 'estimator__max_samples': None,\n",
              " 'estimator__min_impurity_decrease': 0.0,\n",
              " 'estimator__min_samples_leaf': 1,\n",
              " 'estimator__min_samples_split': 2,\n",
              " 'estimator__min_weight_fraction_leaf': 0.0,\n",
              " 'estimator__n_estimators': 100,\n",
              " 'estimator__n_jobs': None,\n",
              " 'estimator__oob_score': False,\n",
              " 'estimator__random_state': None,\n",
              " 'estimator__verbose': 0,\n",
              " 'estimator__warm_start': False,\n",
              " 'n_iter': 10,\n",
              " 'n_jobs': None,\n",
              " 'param_distributions': {'bootstrap': [True, False],\n",
              "  'min_samples_leaf': [1, 2, 4],\n",
              "  'min_samples_split': [2, 5, 10],\n",
              "  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]},\n",
              " 'pre_dispatch': '2*n_jobs',\n",
              " 'random_state': 2021,\n",
              " 'refit': True,\n",
              " 'return_train_score': False,\n",
              " 'scoring': None,\n",
              " 'verbose': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S-k9GYPyc-S"
      },
      "source": [
        "GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlHzUm0iBRN7"
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# params = {'kernel': ['linear', 'rbf', 'poly'],  # 3\n",
        "#           'C': [1, 0.25, 0.5, 0.75],            # 4\n",
        "#           'gamma': ['scale', 'auto', 1, 2, 3],  # 5\n",
        "#           'decision_function_shape': ['ovo','ovr'] # 2\n",
        "#           # 3 * 4 * 5 *2 * 5cv = 120 * 5 = 600 kombinasi\n",
        "# }\n",
        "\n",
        "param_grid = { \n",
        "    'n_estimators': [100,200, 500],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth' : [2,3,4,5,6],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "}\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "cv_test = StratifiedKFold(n_splits=5)\n",
        "clf_grid = GridSearchCV(clf, param_grid, cv=cv_test)\n",
        "clf_grid.fit(train_features, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsN5_mH9C9P_"
      },
      "source": [
        "# from joblib import dump, load\n",
        "\n",
        "# # dump(clf_grid, 'word2vec_svc_tuned.joblib')\n",
        "# # clf_grid = load('word2vec_svc_tuned.joblib')\n",
        "# clf_grid = load('word2vec_svc.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1XVnvhKyhrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cf3eb6-3e81-43dd-dcb0-b5919b4e7e84"
      },
      "source": [
        "clf_grid.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'entropy',\n",
              " 'max_depth': 6,\n",
              " 'max_features': 'auto',\n",
              " 'n_estimators': 500}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBK0SaSVyjk9"
      },
      "source": [
        "%%time\n",
        "\n",
        "# cross-val score\n",
        "scores = cross_val_score(clf_grid, train_features, train_y, cv=5)\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfkpTFaZyln1"
      },
      "source": [
        "# accuracy test\n",
        "y_pred = clf_grid.predict(test_features)\n",
        "\n",
        "accuracy_score(test_y, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0jUqposym5Z"
      },
      "source": [
        "# from joblib import dump, load\n",
        "\n",
        "# dump(clf_grid, 'word2vec_svc_tuned.joblib') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRo6Ida_i9jz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}